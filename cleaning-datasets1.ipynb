{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12477751,"sourceType":"datasetVersion","datasetId":7871951,"isSourceIdPinned":false}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport hashlib, os\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport kagglehub\n\n# Download latest version\npath = kagglehub.dataset_download(\"zlatan599/garbage-dataset-classification\")\nprint(\"Path to dataset files:\", path)\n\nhashes = {}\nduplicates = []\n\ndef file_hash(filePath):\n    with open(filePath, \"rb\") as f: # Open and read files for hashing\n        file_bytes = f.read()\n        return hashlib.md5(file_bytes).hexdigest()\n\nfor root, dirs, files in os.walk(path):\n    for file in files:\n        file_path = os.path.join(root, file)\n        h = file_hash(file_path)\n\n        if h is hashes:\n            print(f\"Duplicate found! Removing {file_path} (same as {hashes[h]})\")\n            os.remove(file_path)\n            duplicates.append((file_path, hashes[h]))\n        else:\n            hashes[h] = file_path\n\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-29T20:09:27.983360Z","iopub.execute_input":"2025-09-29T20:09:27.983559Z"}},"outputs":[{"name":"stdout","text":"Path to dataset files: /kaggle/input/garbage-dataset-classification\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import imagehash\nfrom PIL import Image\n\nimport collections\nhashes = collections.defaultdict(list)\npath = kagglehub.dataset_download(\"zlatan599/garbage-dataset-classification\")\nprint(\"Path to dataset files:\", path)\nvalid_exts = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\"}\n\nfor root, _, files in os.walk(path):\n    for file in files:\n        if os.path.splitext(file)[1].lower() not in valid_exts:\n            continue  # skip non-image files like .csv\n        file_path = os.path.join(root, file)\n        try:\n            with Image.open(file_path) as img:\n                h = imagehash.phash(img)  # perceptual hash\n                hashes[str(h)].append(file_path)\n        except UnidentifiedImageError:\n            print(\"Skipping unreadable file:\", file_path)\nprintf(\"Done cleaning\")\n\n# Show groups of near-duplicates\n#for h, files in hashes.items():\n#    if len(files) > 1:\n#        print(\"Near duplicates:\", files)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}