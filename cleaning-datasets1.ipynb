{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2025-09-29T20:09:27.983559Z","iopub.status.busy":"2025-09-29T20:09:27.983360Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Requirement already satisfied: kagglehub in /Users/miapatrikios/.local/lib/python3.13/site-packages (0.3.13)\n","Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.13/site-packages (from kagglehub) (24.2)\n","Requirement already satisfied: pyyaml in /opt/anaconda3/lib/python3.13/site-packages (from kagglehub) (6.0.2)\n","Requirement already satisfied: requests in /opt/anaconda3/lib/python3.13/site-packages (from kagglehub) (2.32.3)\n","Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.13/site-packages (from kagglehub) (4.67.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.13/site-packages (from requests->kagglehub) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.13/site-packages (from requests->kagglehub) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.13/site-packages (from requests->kagglehub) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.13/site-packages (from requests->kagglehub) (2025.4.26)\n","Note: you may need to restart the kernel to use updated packages.\n","Path to dataset files: /kaggle/input/garbage-dataset-classification\n","Path to dataset files: /kaggle/input/garbage-dataset-classification\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","%pip install kagglehub\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import hashlib, os\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","# Kaggle automatically mounts datasets under /kaggle/input/\n","path = \"/kaggle/input/garbage-dataset-classification\"\n","print(\"Path to dataset files:\", path)\n","print(\"Path to dataset files:\", path)\n","\n","hashes = {}\n","duplicates = []\n","\n","def file_hash(filePath):\n","    with open(filePath, \"rb\") as f: # Open and read files for hashing\n","        file_bytes = f.read()\n","        return hashlib.md5(file_bytes).hexdigest()\n","\n","for root, dirs, files in os.walk(path):\n","    for file in files:\n","        file_path = os.path.join(root, file)\n","        h = file_hash(file_path)\n","\n","        if h is hashes:\n","            print(f\"Duplicate found! Removing {file_path} (same as {hashes[h]})\")\n","            os.remove(file_path)\n","            duplicates.append((file_path, hashes[h]))\n","        else:\n","            hashes[h] = file_path\n","\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":3,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Current working directory: /Users/miapatrikios/cs325/ML-TEAM-15\n","True\n","Path to dataset files: /Users/miapatrikios/cs325/ML-TEAM-15/data/images\n","✅ Found dataset folder with 13901 total files.\n","✅ Done cleaning and converting to RGB.\n","All images saved under: /Users/miapatrikios/cs325/ML-TEAM-15/cleaned/garbage-dataset-rgb\n"]}],"source":["import imagehash\n","from PIL import Image, UnidentifiedImageError\n","import collections\n","\n","hashes = collections.defaultdict(list)\n","\n","import os\n","print(\"Current working directory:\", os.getcwd())\n","\n","\n","# Local input/output paths\n","path = \"data/images\"   # 👈 Correct folder name\n","rgb_file_path = \"cleaned/garbage-dataset-rgb\"\n","print(os.path.exists(path))  # should print True\n","print(\"Path to dataset files:\", os.path.abspath(path))\n","\n","valid_exts = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\"}\n","if not os.path.exists(path):\n","    raise FileNotFoundError(f\"❌ Dataset not found at {path}.\")\n","else:\n","    total_files = sum(len(files) for _, _, files in os.walk(path))\n","    print(f\"✅ Found dataset folder with {total_files} total files.\")\n","for root, _, files in os.walk(path):\n","    # Recreate the subfolder structure inside the output folder\n","    save_dir = os.path.join(rgb_file_path, os.path.relpath(root, path))\n","    os.makedirs(save_dir, exist_ok=True)\n","\n","    for file in files:\n","        if os.path.splitext(file)[1].lower() not in valid_exts:\n","            continue\n","        file_path = os.path.join(root, file)\n","        save_path = os.path.join(save_dir, file)\n","\n","        try:\n","            with Image.open(file_path) as img:\n","                h = imagehash.phash(img)\n","                hashes[str(h)].append(file_path)\n","                img.convert(\"RGB\").save(save_path)\n","        except UnidentifiedImageError:\n","            print(\"Skipping unreadable file:\", file_path)\n","        except Exception as e:\n","            print(f\"Error processing {file_path}: {e}\")\n","\n","print(\"✅ Done cleaning and converting to RGB.\")\n","print(\"All images saved under:\", os.path.abspath(rgb_file_path))\n","\n","# Show groups of near-duplicates\n","#for h, files in hashes.items():\n","#    if len(files) > 1:\n","#        print(\"Near duplicates:\", files)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":7871951,"isSourceIdPinned":false,"sourceId":12477751,"sourceType":"datasetVersion"}],"dockerImageVersionId":31089,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.13.5"}},"nbformat":4,"nbformat_minor":4}
