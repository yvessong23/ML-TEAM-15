{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-29T20:09:27.983559Z",
     "iopub.status.busy": "2025-09-29T20:09:27.983360Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /Users/mp/.cache/kagglehub/datasets/zlatan599/garbage-dataset-classification/versions/5\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import hashlib, os\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"zlatan599/garbage-dataset-classification\")\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "hashes = {}\n",
    "duplicates = []\n",
    "\n",
    "def file_hash(filePath):\n",
    "    with open(filePath, \"rb\") as f: # Open and read files for hashing\n",
    "        file_bytes = f.read()\n",
    "        return hashlib.md5(file_bytes).hexdigest()\n",
    "\n",
    "for root, dirs, files in os.walk(path):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        h = file_hash(file_path)\n",
    "\n",
    "        if h is hashes:\n",
    "            print(f\"Duplicate found! Removing {file_path} (same as {hashes[h]})\")\n",
    "            os.remove(file_path)\n",
    "            duplicates.append((file_path, hashes[h]))\n",
    "        else:\n",
    "            hashes[h] = file_path\n",
    "\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ImageHash in ./.venv/lib/python3.9/site-packages (4.3.2)\n",
      "Requirement already satisfied: PyWavelets in ./.venv/lib/python3.9/site-packages (from ImageHash) (1.6.0)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.9/site-packages (from ImageHash) (2.0.2)\n",
      "Requirement already satisfied: pillow in ./.venv/lib/python3.9/site-packages (from ImageHash) (11.3.0)\n",
      "Requirement already satisfied: scipy in ./.venv/lib/python3.9/site-packages (from ImageHash) (1.13.1)\n",
      "Path to dataset files: /Users/mp/.cache/kagglehub/datasets/zlatan599/garbage-dataset-classification/versions/5\n",
      "Done cleaning\n"
     ]
    }
   ],
   "source": [
    "!pip install ImageHash\n",
    "import imagehash\n",
    "from PIL import Image\n",
    "\n",
    "import collections\n",
    "hashes = collections.defaultdict(list)\n",
    "path = kagglehub.dataset_download(\"zlatan599/garbage-dataset-classification\")\n",
    "print(\"Path to dataset files:\", path)\n",
    "valid_exts = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\"}\n",
    "\n",
    "for root, _, files in os.walk(path):\n",
    "    for file in files:\n",
    "        if os.path.splitext(file)[1].lower() not in valid_exts:\n",
    "            continue  # skip non-image files like .csv\n",
    "        file_path = os.path.join(root, file)\n",
    "        try:\n",
    "            with Image.open(file_path) as img:\n",
    "                h = imagehash.phash(img)  # perceptual hash\n",
    "                hashes[str(h)].append(file_path)\n",
    "        except UnidentifiedImageError:\n",
    "            print(\"Skipping unreadable file:\", file_path)\n",
    "print(\"Done cleaning\")\n",
    "\n",
    "# Show groups of near-duplicates\n",
    "#for h, files in hashes.items():\n",
    "#    if len(files) > 1:\n",
    "#        print(\"Near duplicates:\", files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nia Maheshwari\n",
    "# one-hot encode function \n",
    "\n",
    "# get all the unique possible values in a column to encode (we will create a new column for all unique vals - 1)\n",
    "# go through every row and see if the value of that category is that column name (add 1)\n",
    "def PandasOneHotEncodeNumpy(DataFrame, Columns):\n",
    "    names = []\n",
    "    for col in Columns:\n",
    "        features = []\n",
    "        features = DataFrame[col].unique()\n",
    "        features = features[:-1]\n",
    "        \n",
    "        for feat in features:\n",
    "            DataFrame[feat] = DataFrame[col].apply(lambda x: 1 if x==feat else 0)\n",
    "            names.append(feat)\n",
    "    DataFrame = DataFrame.drop(columns=Columns)\n",
    "    return DataFrame,names  "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7871951,
     "isSourceIdPinned": false,
     "sourceId": 12477751,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
