{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yvessong23/ML-TEAM-15/blob/main/BinBuddiesCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QDz947BEKBtG"
      },
      "outputs": [],
      "source": [
        "# Import TensorFlow\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report, precision_score, recall_score, f1_score, roc_auc_score\n",
        "import seaborn as sns\n",
        "import torchvision.transforms as transforms #missing run pip install torchvision\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Randomly generates a new augmented image based on the input\n",
        "\n",
        "# Random generator\n",
        "rng1 = np.random.RandomState(seed=123)\n",
        "\n",
        "def RandAugment(inpt_img:Image.Image) -> Image.Image:\n",
        "\n",
        "    augment_select = rng1.random_integers(0,5)\n",
        "    \n",
        "    match augment_select:\n",
        "\n",
        "        case 0:\n",
        "            new_img = inpt_img.transform(Image.Transpose.ROTATE_90)\n",
        "        case 1:\n",
        "            new_img = inpt_img.transpose(Image.Transpose.ROTATE_180)\n",
        "        case 2:\n",
        "            new_img = inpt_img.transpose(Image.Transpose.ROTATE_270)\n",
        "        case 3:\n",
        "            new_img = inpt_img.transpose(Image.Transpose.FLIP_LEFT_RIGHT)\n",
        "        case 4:\n",
        "            new_img = inpt_img.transpose(Image.Transpose.FLIP_TOP_BOTTOM)\n",
        "        case 5:\n",
        "            rand_bright = rng1.random_integers(0,100) / 100\n",
        "            rand_contrast = rng1.random_integers(0,100) / 100\n",
        "            rand_staturation = rng1.random_integers(0,100) / 100\n",
        "            rand_hue = rng1.random_integers(0,50) / 100\n",
        "\n",
        "            color_jitter = transforms.ColorJitter(brightness=rand_bright, contrast=rand_contrast, saturation=rand_staturation, hue=rand_hue)\n",
        "            \n",
        "            new_img = color_jitter(inpt_img)\n",
        "        case _:\n",
        "            print(\"Something went wrong\")\n",
        "\n",
        "    return new_img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "WJM9MyG1JvjH",
        "outputId": "6b5b6d01-c929-4c23-98cc-720dbd3d5a79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in c:\\users\\foces\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.3.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: kagglehub in c:\\users\\foces\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.3.13)\n",
            "Requirement already satisfied: packaging in c:\\users\\foces\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from kagglehub) (24.2)\n",
            "Requirement already satisfied: pyyaml in c:\\users\\foces\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from kagglehub) (6.0.2)\n",
            "Requirement already satisfied: requests in c:\\users\\foces\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from kagglehub) (2.32.3)\n",
            "Requirement already satisfied: tqdm in c:\\users\\foces\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from kagglehub) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\foces\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->kagglehub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\foces\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->kagglehub) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\foces\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->kagglehub) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\foces\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->kagglehub) (2024.7.4)\n",
            "Requirement already satisfied: colorama in c:\\users\\foces\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm->kagglehub) (0.4.6)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Path to dataset files: C:\\Users\\Foces\\.cache\\kagglehub\\datasets\\zlatan599\\garbage-dataset-classification\\versions\\5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Foces\\AppData\\Local\\Temp\\ipykernel_25260\\1826601181.py:8: DeprecationWarning: This function is deprecated. Please call randint(0, 5 + 1) instead\n",
            "  augment_select = rng1.random_integers(0,5)\n"
          ]
        }
      ],
      "source": [
        "%pip install numpy\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import hashlib, os\n",
        "from PIL import Image\n",
        "%pip install kagglehub\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"zlatan599/garbage-dataset-classification\")\n",
        "rgb_path = \"cleaned/garbage-dataset-rgb\"\n",
        "print(\"Path to dataset files:\", path)\n",
        "\n",
        "hashes = {}\n",
        "duplicates = []\n",
        "\n",
        "def file_hash(filePath):\n",
        "    with open(filePath, \"rb\") as f: # Open and read files for hashing\n",
        "        file_bytes = f.read()\n",
        "        return hashlib.md5(file_bytes).hexdigest()\n",
        "\n",
        "\n",
        "for root, dirs, files in os.walk(path):\n",
        "    \n",
        "    augment = 0 # if augment is 0, will generate an augmented version of the image current file (ADDED)\n",
        "\n",
        "    for file in files:\n",
        "\n",
        "        file_path = os.path.join(root, file)\n",
        "        h = file_hash(file_path)\n",
        "\n",
        "        if h in hashes:\n",
        "            print(f\"Duplicate found! Removing {file_path} (same as {hashes[h]})\")\n",
        "            try:\n",
        "                os.remove(file_path)\n",
        "                duplicates.append((file_path, hashes[h]))\n",
        "            except OSError as e:\n",
        "                print(f\"Warning: Could not remove {file_path} due to read-only filesystem. Error: {e}\")\n",
        "        else:\n",
        "            hashes[h] = file_path\n",
        "            try:\n",
        "                # Preserve the dataset’s subfolder structure\n",
        "                rel_dir = os.path.relpath(root, path)\n",
        "                save_dir = os.path.join(rgb_path, rel_dir)\n",
        "                os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "                save_path = os.path.join(save_dir, file)\n",
        "\n",
        "                # Skip non-image files (like metadata.csv)\n",
        "                if not file.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\")):\n",
        "                    continue\n",
        "\n",
        "\n",
        "                augment = (augment + 1) % 2 #Used to determine if current image should get an augmented version (ADDED)\n",
        "\n",
        "                with Image.open(file_path) as img:\n",
        "\n",
        "                    if augment == 0:                                # ADDED CODE START\n",
        "                        file_name, file_extension = file.split('.')\n",
        "                        new_name = file_name + \"_A.\" + file_extension \n",
        "\n",
        "                        augment_img = RandAugment(img)\n",
        "                        augment_save = os.path.join(save_dir, new_name)\n",
        "\n",
        "                        augment_img.convert(\"RGB\").save(augment_save)   #ADDED CODE END\n",
        "\n",
        "                    img.convert(\"RGB\").save(save_path)\n",
        "                    img = Image.open(file_path)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {file_path}: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "o2PHRTnEJ9Zf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cardboard: 1718 images\n",
            "glass: 2293 images\n",
            "metal: 1628 images\n",
            "paper: 1809 images\n",
            "plastic: 1709 images\n",
            "trash: 2500 images\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow.keras import models, layers, regularizers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "rgb_path = \"cleaned/garbage-dataset-rgb\"\n",
        "base_path = os.path.join(rgb_path, \"Garbage_Dataset_Classification\", \"images\")\n",
        "DATASET_DIR = base_path\n",
        "\n",
        "# Set up class names\n",
        "class_names = ['cardboard', 'glass', 'metal', 'paper', 'plastic', 'trash']\n",
        "num_classes = len(class_names)\n",
        "\n",
        "# Ensure 'duplicates' is defined. It is expected to be populated by cell WJM9MyG1JvjH.\n",
        "# If cell WJM9MyG1JvjH hasn't been run, 'duplicates' will be an empty list, and thus\n",
        "# no files will be filtered as duplicates in this cell.\n",
        "if 'duplicates' not in globals():\n",
        "    duplicates = []\n",
        "\n",
        "# Verify and collect all image paths and their labels\n",
        "image_paths = []\n",
        "labels = []\n",
        "\n",
        "duplicate_filepaths = {f[0] for f in duplicates} # Extract only the file paths from the duplicates list\n",
        "\n",
        "for class_idx, class_name in enumerate(class_names):\n",
        "    class_dir = os.path.join(base_path, class_name)\n",
        "    if os.path.exists(class_dir):\n",
        "        class_images = [f for f in os.listdir(class_dir)\n",
        "                       if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.tiff'))]\n",
        "        print(f\"{class_name}: {len(class_images)} images\")\n",
        "\n",
        "        for img_file in class_images:\n",
        "            full_path = os.path.join(class_dir, img_file)\n",
        "            if full_path not in duplicate_filepaths: # Only add if not a duplicate\n",
        "                image_paths.append(full_path)\n",
        "                labels.append(class_idx)\n",
        "    else:\n",
        "        print(f\"Warning: {class_dir} does not exist\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hyper parameter Testing "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameters for searching with Optuna\n",
        "BLOCK_NUM = [3,4,5]\n",
        "CONV_LAYERS = [1, 2, 3]\n",
        "BATCH_SIZES = [16, 32, 64]\n",
        "EPOCHS = [10, 20, 30]\n",
        "CONV_LAYER_SIZE_BASE = [32, 64, 128]\n",
        "FILTERS_SIZE = [3,5,7]\n",
        "REGULARIZATION_CONST = [0.001, 0.01, 0.1]\n",
        "REG_TYPE = [\"n/a\", \"L2\", \"Dropout\", \"Both\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: optuna in ./.venv/lib/python3.9/site-packages (4.6.0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in ./.venv/lib/python3.9/site-packages (from optuna) (1.16.5)\n",
            "Requirement already satisfied: colorlog in ./.venv/lib/python3.9/site-packages (from optuna) (6.10.1)\n",
            "Requirement already satisfied: numpy in ./.venv/lib/python3.9/site-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.9/site-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in ./.venv/lib/python3.9/site-packages (from optuna) (2.0.44)\n",
            "Requirement already satisfied: tqdm in ./.venv/lib/python3.9/site-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in ./.venv/lib/python3.9/site-packages (from optuna) (6.0.3)\n",
            "Requirement already satisfied: Mako in ./.venv/lib/python3.9/site-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in ./.venv/lib/python3.9/site-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
            "Requirement already satisfied: tomli in ./.venv/lib/python3.9/site-packages (from alembic>=1.5.0->optuna) (2.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in ./.venv/lib/python3.9/site-packages (from Mako->alembic>=1.5.0->optuna) (3.0.3)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Found 8163 images belonging to 6 classes.\n",
            "Found 2328 images belonging to 6 classes.\n"
          ]
        }
      ],
      "source": [
        "%pip install optuna\n",
        "import optuna\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import models, layers, regularizers\n",
        "\n",
        "IMG_HEIGHT = 256\n",
        "IMG_WIDTH = 256\n",
        "IMG_SIZE = (IMG_HEIGHT, IMG_WIDTH)\n",
        "BATCH_SIZE = 32\n",
        "from tensorflow.keras import models, layers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "IMG_HEIGHT = 256\n",
        "IMG_WIDTH = 256\n",
        "IMG_SIZE   = (IMG_HEIGHT, IMG_WIDTH)\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    validation_split=0.3\n",
        ")\n",
        "\n",
        "val_test_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    DATASET_DIR,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    subset='training',\n",
        "    shuffle=True,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "# Load all validation images into memory, then split into val + test\n",
        "val_generator_full = val_test_datagen.flow_from_directory(\n",
        "    DATASET_DIR,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=1,\n",
        "    class_mode='categorical',\n",
        "    subset='validation',\n",
        "    shuffle=False,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "val_data = []\n",
        "val_labels = []\n",
        "\n",
        "for i in range(len(val_generator_full)):\n",
        "    x, y = val_generator_full[i]\n",
        "    val_data.append(x[0])\n",
        "    val_labels.append(y[0])\n",
        "\n",
        "val_data = np.array(val_data)\n",
        "val_labels = np.array(val_labels)\n",
        "\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    val_data,\n",
        "    val_labels,\n",
        "    test_size=0.5,\n",
        "    random_state=42,\n",
        "    stratify=np.argmax(val_labels, axis=1)\n",
        ")\n",
        "\n",
        "val_generator_data = (X_val, y_val)\n",
        "test_generator_data = (X_test, y_test)\n",
        "\n",
        "\n",
        "\n",
        "def build_model(trial):\n",
        "\n",
        "    block_num   = trial.suggest_categorical(\"block_num\", [3, 4, 5])\n",
        "    conv_layers = trial.suggest_categorical(\"conv_layers\", [1, 2, 3])\n",
        "    batch_size  = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n",
        "    epochs      = trial.suggest_categorical(\"epochs\", [10, 20, 30])\n",
        "    reg_type    = trial.suggest_categorical(\"reg_type\", [\"n/a\", \"L2\", \"Dropout\", \"Both\"])\n",
        "\n",
        "    # example: choose regularizer based on reg_type\n",
        "    if reg_type in [\"L2\", \"Both\"]:\n",
        "        reg = regularizers.l2(trial.suggest_float(\"l2_reg\", 1e-5, 1e-2, log=True))\n",
        "    else:\n",
        "        reg = None\n",
        "\n",
        "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5) if reg_type in [\"Dropout\", \"Both\"] else 0.0\n",
        "\n",
        "    # Hyperparameters to tune\n",
        "    f1 = trial.suggest_categorical(\"f1\", [32, 48, 64])\n",
        "    f2 = trial.suggest_categorical(\"f2\", [64, 96, 128])\n",
        "    f3 = trial.suggest_categorical(\"f3\", [128, 192, 256])\n",
        "    f4 = trial.suggest_categorical(\"f4\", [256, 384, 512])\n",
        "\n",
        "    d1 = trial.suggest_float(\"d1\", 0.1, 0.4)\n",
        "    d2 = trial.suggest_float(\"d2\", 0.1, 0.4)\n",
        "    d3 = trial.suggest_float(\"d3\", 0.2, 0.5)\n",
        "    d4 = trial.suggest_float(\"d4\", 0.2, 0.5)\n",
        "\n",
        "    dense_units = trial.suggest_categorical(\"dense_units\", [128, 256, 512])\n",
        "    l2_reg = trial.suggest_float(\"l2_reg\", 1e-5, 1e-2, log=True)\n",
        "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
        "\n",
        "    kernel = trial.suggest_categorical(\"kernel\", [3, 5])\n",
        "\n",
        "    model = models.Sequential([\n",
        "        layers.Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n",
        "\n",
        "        layers.Conv2D(f1, (kernel, kernel), padding='same', activation='relu'),\n",
        "        layers.Conv2D(f2, (kernel, kernel), padding='same', activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(d1),\n",
        "        layers.MaxPool2D(2),\n",
        "\n",
        "        layers.Conv2D(f3, (kernel, kernel), padding='same', activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(d2),\n",
        "        layers.MaxPool2D(2),\n",
        "\n",
        "        layers.Conv2D(f4, (3,3), padding='same', activation='relu',\n",
        "                      kernel_regularizer=regularizers.l2(l2_reg)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(d3),\n",
        "        layers.MaxPool2D(2),\n",
        "\n",
        "        layers.GlobalAveragePooling2D(),\n",
        "\n",
        "        layers.Dense(dense_units, activation='relu',\n",
        "                     kernel_regularizer=regularizers.l2(l2_reg)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(d4),\n",
        "\n",
        "        layers.Dense(6, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def objective(trial):\n",
        "    model = build_model(trial)\n",
        "\n",
        "    history = model.fit(\n",
        "        train_generator,\n",
        "        validation_data=val_generator_data,\n",
        "        epochs=trial.params[\"epochs\"],\n",
        "        batch_size=trial.params[\"batch_size\"],\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    return history.history[\"val_loss\"][-1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-11-19 17:27:39,288] A new study created in memory with name: no-name-741c3b2f-67b4-43b6-959a-37c98dbc6195\n",
            "/Users/mp/ML-TEAM-15/.venv/lib/python3.9/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        }
      ],
      "source": [
        "study = optuna.create_study(direction=\"minimize\")\n",
        "study.optimize(objective, n_trials=30)\n",
        "\n",
        "print(\"Best params:\", study.best_params)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Base Model Testing "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8iVV25H9RXYy"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import models, layers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "IMG_HEIGHT = 256\n",
        "IMG_WIDTH = 256\n",
        "IMG_SIZE   = (IMG_HEIGHT, IMG_WIDTH)\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    validation_split=0.3\n",
        ")\n",
        "\n",
        "val_test_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    DATASET_DIR,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    subset='training',\n",
        "    shuffle=True,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "# Load all validation images into memory, then split into val + test\n",
        "val_generator_full = val_test_datagen.flow_from_directory(\n",
        "    DATASET_DIR,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=1,\n",
        "    class_mode='categorical',\n",
        "    subset='validation',\n",
        "    shuffle=False,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "val_data = []\n",
        "val_labels = []\n",
        "\n",
        "for i in range(len(val_generator_full)):\n",
        "    x, y = val_generator_full[i]\n",
        "    val_data.append(x[0])\n",
        "    val_labels.append(y[0])\n",
        "\n",
        "val_data = np.array(val_data)\n",
        "val_labels = np.array(val_labels)\n",
        "\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    val_data,\n",
        "    val_labels,\n",
        "    test_size=0.5,\n",
        "    random_state=42,\n",
        "    stratify=np.argmax(val_labels, axis=1)\n",
        ")\n",
        "\n",
        "val_generator_data = (X_val, y_val)\n",
        "test_generator_data = (X_test, y_test)\n",
        "\n",
        "model_gap = models.Sequential([\n",
        "    layers.Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n",
        "\n",
        "    # Block 1\n",
        "    layers.Conv2D(32, (3,3), padding='same', activation='relu'),\n",
        "    layers.Conv2D(64, (3,3), padding='same', activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.25),\n",
        "    layers.MaxPool2D(2),\n",
        "\n",
        "    # Block 2\n",
        "    layers.Conv2D(128, (5,5), padding='same', activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.25),\n",
        "    layers.MaxPool2D(2),\n",
        "\n",
        "    # Block 3\n",
        "    layers.Conv2D(256, (3,3), padding='same', activation='relu',\n",
        "    kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.35),\n",
        "    layers.MaxPool2D(2),\n",
        "\n",
        "    # Block 4\n",
        "    layers.Conv2D(512, (3,3), padding='same', activation='relu',\n",
        "    kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.35),\n",
        "    layers.MaxPool2D(2),\n",
        "\n",
        "    # Head with GAP\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.50),\n",
        "    layers.Dense(6, activation='softmax'),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8P2-UQ9WUS1b"
      },
      "source": [
        "# Testing CNN with Global Average Pooling\n",
        "\n",
        "Using ChatGPT and Kaggle, I found that GAP or Global Average Pooling can significantly reduce the number of parameters. This method compresses each feature map into a single value by averaging its activations (outputs). This reduced our dimensionality from 5+ million to about 56K. This also allows the model to generalize better and train faster.\n",
        "https://www.kaggle.com/code/faressayah/cifar-10-images-classification-using-cnns-88\n",
        "https://www.kaggle.com/code/zlatan599/f1-0-97-garbage-classif-final-version-2\n",
        "\n",
        "The baseline run of this model with no dropout, data augmentation, or hyperparameter tuning was:\n",
        "\n",
        "* Train Accuracy: ~0.76\n",
        "\n",
        "* Train Loss: ~0.81\n",
        "\n",
        "* Best Validation Accuracy: ~0.59 (epoch 27)\n",
        "\n",
        "* Best Validation Loss: ~1.43\n",
        "\n",
        "* Test Accuracy: 0.5902\n",
        "\n",
        "* Test Loss: 1.52\n",
        "\n",
        "* Training Time: ≈ 24 minutes for 30 epochs on GPU\n",
        "\n",
        "# Strong classes:\n",
        "\n",
        "Cardboard: 0.74 F1 (0.71 precision / 0.77 recall)\n",
        "\n",
        "Paper: 0.68 F1\n",
        "\n",
        "Trash: 0.64 F1\n",
        "\n",
        "# Medium:\n",
        "\n",
        "Glass: 0.61 F1\n",
        "\n",
        "# Weak:\n",
        "\n",
        "Plastic: 0.50 F1\n",
        "\n",
        "Metal: 0.40 F1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGP6Lg4EURjz"
      },
      "outputs": [],
      "source": [
        "print(tf.config.list_physical_devices(\"GPU\"))\n",
        "\n",
        "model_gap.compile(\n",
        "    optimizer=Adam(learning_rate=1e-3),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy'],\n",
        ")\n",
        "\n",
        "model_gap.summary()\n",
        "\n",
        "checkpoint = ModelCheckpoint(\n",
        "    'best_model_gap.keras',\n",
        "    monitor='val_loss',\n",
        "    save_best_only=True,\n",
        "    mode='min',\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "history_gap = model_gap.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator_data,\n",
        "    epochs=30,\n",
        "    callbacks=[checkpoint]\n",
        ")\n",
        "\n",
        "test_loss, test_acc = model_gap.evaluate(\n",
        "    X_test,\n",
        "    y_test,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    verbose=2\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1vtq0ZFLozs"
      },
      "source": [
        "# Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ts41qq8wL2T8"
      },
      "source": [
        "Model Evaluation at baseline\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Comparing GAP vs Flatten Head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRfEKcsPCpSI"
      },
      "outputs": [],
      "source": [
        "test_images, test_labels = test_generator_data\n",
        "\n",
        "y_probs = model_gap.predict(test_images)\n",
        "\n",
        "y_pred = np.argmax(y_probs, axis=1)\n",
        "y_true = np.argmax(test_labels, axis=1)\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=class_names))\n",
        "\n",
        "precision_macro = precision_score(y_true, y_pred, average='macro')\n",
        "recall_macro = recall_score(y_true, y_pred, average='macro')\n",
        "f1_macro = f1_score(y_true, y_pred, average='macro')\n",
        "\n",
        "precision_weighted = precision_score(y_true, y_pred, average='weighted')\n",
        "recall_weighted = recall_score(y_true, y_pred, average='weighted')\n",
        "f1_weighted = f1_score(y_true, y_pred, average='weighted')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzOtLVIpMjto"
      },
      "outputs": [],
      "source": [
        "# Confusion Matrix Visualization\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "plt.figure(figsize=(7,6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=class_names,\n",
        "            yticklabels=class_names)\n",
        "plt.title(\"Confusion Matrix (CNN + GAP)\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AgW-Lze1PGFI"
      },
      "outputs": [],
      "source": [
        "# ROC Curves\n",
        "\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "\n",
        "for i, class_name in enumerate(class_names):\n",
        "    fpr, tpr, _ = roc_curve(y_true == i, y_probs[:, i])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, lw=2, label=f\"{class_name} (AUC = {roc_auc:.3f})\")\n",
        "\n",
        "plt.plot([0,1], [0,1], 'k--')\n",
        "plt.title(\"ROC Curves (One-vs-Rest)\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2CUvDJhPMAu"
      },
      "outputs": [],
      "source": [
        "# Precision–Recall Curves\n",
        "\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "\n",
        "for i, class_name in enumerate(class_names):\n",
        "    precision, recall, _ = precision_recall_curve(y_true == i, y_probs[:, i])\n",
        "    plt.plot(recall, precision, lw=2, label=f\"{class_name}\")\n",
        "\n",
        "plt.title(\"Precision–Recall Curves\")\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yJ0bcMvKYDa"
      },
      "source": [
        "## Loss Curve Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XX5rgaWtKYDb"
      },
      "outputs": [],
      "source": [
        "train_loss = history_gap.history['loss']\n",
        "val_loss   = history_gap.history['val_loss']\n",
        "train_acc  = history_gap.history['accuracy']\n",
        "val_acc    = history_gap.history['val_accuracy']\n",
        "\n",
        "epochs = range(1, len(train_loss) + 1)\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(15,5))\n",
        "\n",
        "# LOSS\n",
        "ax[0].plot(epochs, train_loss, 'b-', label='Training Loss', linewidth=2)\n",
        "ax[0].plot(epochs, val_loss, 'r-', label='Validation Loss', linewidth=2)\n",
        "ax[0].set_title(\"Loss Curve\", fontsize=15)\n",
        "ax[0].set_xlabel(\"Epochs\")\n",
        "ax[0].set_ylabel(\"Loss\")\n",
        "ax[0].legend()\n",
        "ax[0].grid(True)\n",
        "\n",
        "# showing final test loss on chart\n",
        "ax[0].text(0.95, 0.95, f\"Test Loss: {test_loss:.4f}\",\n",
        "           ha='right', va='top', transform=ax[0].transAxes,\n",
        "           fontsize=12, color='black',\n",
        "           bbox=dict(facecolor='white', alpha=0.7, edgecolor='gray'))\n",
        "\n",
        "# ACCURACY\n",
        "ax[1].plot(epochs, train_acc, 'b-', label='Training Accuracy', linewidth=2)\n",
        "ax[1].plot(epochs, val_acc, 'r-', label='Validation Accuracy', linewidth=2)\n",
        "ax[1].set_title(\"Accuracy Curve\", fontsize=15)\n",
        "ax[1].set_xlabel(\"Epochs\")\n",
        "ax[1].set_ylabel(\"Accuracy\")\n",
        "ax[1].legend()\n",
        "ax[1].grid(True)\n",
        "\n",
        "\n",
        "ax[1].text(0.95, 0.95, f\"Test Acc: {test_acc:.4f}\",\n",
        "           ha='right', va='top', transform=ax[1].transAxes,\n",
        "           fontsize=12, color='black',\n",
        "           bbox=dict(facecolor='white', alpha=0.7, edgecolor='gray'))\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
